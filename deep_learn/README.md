# Satellite-Based Crop Classification: Deep Learning and Machine Learning

## Project Overview

Crop type classification using Sentinel-2 multispectral time-series imagery for the Western Cape of South Africa. Based on the [Radiant Earth Spot The Crop Challenge](https://github.com/radiantearth/spot-the-crop-challenge).

**Crop classes**: Lucerne/Medics, Small grain grazing, Barley, Canola, Wheat, and others (9 total).

Three modeling approaches are benchmarked:

- **Classical ML** (field-level): XGBoost, LightGBM, Random Forest, Logistic Regression, Voting/Stacking ensembles
- **Deep Learning** (pixel/field-level): CNN+BiLSTM with Focal Loss, TabTransformer (TabNet), 1D CNN
- **Patch-Level DL**: 3D CNN, Multi-Channel CNN, Transformer, Ensemble architectures

All models aggregate predictions to the field level (group by FID, majority vote or mean pooling) to avoid data leakage.

## Data Access

Datasets are hosted on Google Drive:

```bash
pip install gdown

# Classical ML models (xr_fresh features, ~189 columns)
gdown https://drive.google.com/uc?id=1tywwZdycgBTKgkNAZMKyCL5skaezzE3O -O src/Data/final_data.parquet

# Deep Learning pixel-level models (per-band values)
gdown https://drive.google.com/uc?id=1QFCA6AIF85MtbO4oDtA_64I8GwjEZJd8 -O src/merged_dl_258_259.parquet

# Patch-level models: run Create_Patches.py then Create Master Data.py to generate patch data
```

> **Note:** Place data files in the locations expected by `src/config.py`, or edit the paths there.

## Repository Structure

```text
deep_learn/
├── README.md
├── SETUP.md                          # Installation and run guide
├── install.sh                        # Auto-detects CUDA, installs PyTorch + TensorFlow
├── requirements.txt
│
├── src/
│   ├── config.py                     # All data/model/output paths
│   ├── report.py                     # Standardized model reporting (ModelReport class)
│   ├── compare_models.py             # Cross-model comparison tool
│   ├── run_all_dl_models.py          # Run all DL models sequentially
│   │
│   ├── Classical Machine Learning/
│   │   ├── Field Level/
│   │   │   ├── xg_boost_hyper.py         # XGBoost + Optuna (75 trials)
│   │   │   ├── Ensemble - Voting and Stacking.py  # Voting + Stacking ensembles
│   │   │   ├── SMOTE_meta.py             # SMOTE oversampling + meta-learner
│   │   │   ├── inference_classical_ensemble.py
│   │   │   └── EDA Field Level.py
│   │   └── pixel_level/
│   │       ├── base_ml_models.py         # LR, RF, LightGBM, XGBoost (pixel-level)
│   │       └── pixel_voting.py
│   │
│   ├── Deep Learning/
│   │   ├── Pixel_Field_Level/
│   │   │   ├── cnn_bilstm.py             # CNN+BiLSTM ensemble (best model)
│   │   │   ├── field_acc_cnnlstm.py      # Field-level eval of CNN+BiLSTM
│   │   │   ├── TabTransformer_Final_Field.py  # TabNet ensemble (field-level)
│   │   │   ├── TabTransformer.py         # TabNet (pixel-level)
│   │   │   ├── cnn_dl_hyper.py           # 1D CNN Optuna search
│   │   │   └── best_ccn_params.py        # 1D CNN with best params
│   │   └── Patch Level/
│   │       ├── Create_Patches.py         # Patch creation from field boundaries
│   │       ├── Create Master Data.py     # Pixel extraction per patch
│   │       ├── 3D_CNN.py                 # 3D CNN (spectral-temporal)
│   │       ├── Multi_Channel_CNN.py
│   │       ├── Ensemble - 3D CNN.py
│   │       └── Inference_Ensemble.py
│   │
│   └── reports/                          # Auto-generated model reports
│       ├── XGBoost_Field-Level_20260204_153012/
│       │   ├── report.pdf
│       │   ├── metadata.json
│       │   ├── metrics.csv
│       │   ├── per_class_metrics.csv
│       │   ├── confusion_matrix.csv
│       │   ├── confusion_matrix.png
│       │   ├── feature_importance.png
│       │   └── predictions.csv
│       ├── CNN-BiLSTM_Ensemble_20260204_161530/
│       │   └── ...
│       ├── model_comparison.pdf          # Generated by compare_models.py
│       └── model_comparison.csv
│
└── ../out_of_sample/                     # Holdout test region inference
    ├── combine_test_parquets.py          # Merge xr_fresh parquets for Classical ML
    ├── inference_classical_ml.py         # Voting + Stacking predictions
    ├── inference_cnn_bilstm.py           # CNN-BiLSTM ensemble predictions
    ├── inference_tabnet.py               # TabNet ensemble predictions
    ├── create_test_patches.py            # Generate test patch geometries
    ├── inference_3d_cnn.py               # 3D CNN predictions
    └── compare_predictions.py            # Compare models, create submission
```

## Key Models

### Classical ML (Field-Level)

| Model             | Script                               | Notes                                          |
| ----------------- | ------------------------------------ | ---------------------------------------------- |
| XGBoost           | `xg_boost_hyper.py`                  | Optuna tuning (75 trials), feature importance  |
| Voting Ensemble   | `Ensemble - Voting and Stacking.py`  | LR + RF + HGB + XGB, soft voting               |
| Stacking Ensemble | `Ensemble - Voting and Stacking.py`  | Same base learners, LR meta-learner            |
| SMOTE + Meta      | `SMOTE_meta.py`                      | SMOTE oversampling for class imbalance         |

### Deep Learning (Pixel/Field-Level)

| Model          | Script                                       | Notes                                              |
| -------------- | -------------------------------------------- | -------------------------------------------------- |
| CNN+BiLSTM     | `cnn_bilstm.py`                              | 5-seed ensemble, Focal Loss, **best overall**      |
| TabTransformer | `TabTransformer_Final_Field.py`              | TabNet 5-model ensemble, field-level aggregation   |
| 1D CNN         | `cnn_dl_hyper.py` / `best_ccn_params.py`     | Optuna hyperparameter search                       |

### Patch-Level DL

| Model             | Script                 | Notes                                             |
| ----------------- | ---------------------- | ------------------------------------------------- |
| 3D CNN            | `3D_CNN.py`            | Spectral-temporal convolutions on 128x128 patches |
| Multi-Channel CNN | `Multi_Channel_CNN.py` | Spatial texture learning                          |
| Ensemble          | `Ensemble - 3D CNN.py` | Combines patch-level models                       |

## Results Summary

| Approach       | Cohen's Kappa | F1 Weighted | Key Highlights                                  |
| -------------- | ------------- | ----------- | ----------------------------------------------- |
| Classical ML   | ~0.69         | ~0.77       | Voting Ensemble highest among classical models  |
| Deep Learning  | ~0.77         | ~0.84       | CNN+BiLSTM+Focal Loss strongest overall         |
| Patch-Level DL | ~0.66         | ~0.75       | Ensemble of CNN+Transformer best patch-level    |

## Model Reporting

Every training script auto-generates a standardized report on completion. Each run creates a timestamped folder under `src/reports/` containing:

| File                     | Description                                                         |
| ------------------------ | ------------------------------------------------------------------- |
| `report.pdf`             | Multi-page PDF: summary, per-class metrics, confusion matrix, etc.  |
| `metadata.json`          | Machine-readable metrics, hyperparameters, split info, class names  |
| `metrics.csv`            | Single row: accuracy, Cohen's Kappa, F1 weighted, F1 macro          |
| `per_class_metrics.csv`  | Per-class precision, recall, F1, support                            |
| `confusion_matrix.csv`   | NxN labeled confusion matrix                                        |
| `confusion_matrix.png`   | Heatmap at 300 dpi                                                  |
| `feature_importance.png` | Top-20 features (tree models only)                                  |
| `training_curves.png`    | Loss/accuracy over epochs (DL models only)                          |
| `predictions.csv`        | Field-level true vs. predicted labels                               |

### Comparing Models

After running multiple models, generate a cross-model comparison:

```bash
cd src
python compare_models.py
```

This scans `reports/*/metadata.json` and produces `reports/model_comparison.pdf` (table + bar chart) and `reports/model_comparison.csv`.

### Model Registry

All trained models are automatically registered in `src/model_registry.json` when reports are generated. Use the registry to track runs and find best models:

```bash
cd src

# List all registered models
python model_registry.py list

# List models of a specific type
python model_registry.py list --name "CNN-BiLSTM"

# Get best model by metric
python model_registry.py best --name "CNN-BiLSTM" --metric kappa

# Import existing reports into registry
python model_registry.py import-reports
```

Programmatic usage:

```python
from model_registry import ModelRegistry

registry = ModelRegistry()

# Get best CNN-BiLSTM run
best = registry.get_best("CNN-BiLSTM", metric="kappa")
print(best["run_id"], best["metrics"])

# Check if artifacts still exist
status = registry.check_artifacts_exist(best["run_id"])
```

### Reporting API

Reports are generated via the `ModelReport` class in `src/report.py`:

```python
from report import ModelReport

report = ModelReport("My Model Name")
report.set_hyperparameters({"lr": 1e-3, "epochs": 25})
report.set_split_info(train=5000, val=500, test=1000, seed=42)
report.set_metrics(y_test, y_pred, class_names)
report.set_predictions(field_ids, y_test, y_pred, class_names)     # optional
report.set_feature_importance(importances, feature_names)           # optional
report.set_training_history(history.history)                        # optional (Keras)
report.generate()
```

## Installation

See [SETUP.md](SETUP.md) for full details. Quick start:

```bash
cd deep_learn
bash install.sh     # auto-detects CUDA, installs PyTorch + TensorFlow with GPU support
```

To force a specific CUDA variant: `bash install.sh cu126` (or `cu124`, `cu121`, `cu118`, `cpu`).

Edit `src/config.py` to set your data and output paths before running any models.

## Running Models

All commands run from `deep_learn/src/`. Each script trains, evaluates, and auto-generates a report in `reports/`.

### Stage 1: Classical ML (Field-Level)

**Data required:** `Data/final_data.parquet`

```bash
# XGBoost with Optuna hyperparameter tuning (75 trials)
# Outputs: best params, feature importance, field-level predictions
# Report includes: feature_importance.png, predictions.csv
python "Classical Machine Learning/Field Level/xg_boost_hyper.py"

# Voting + Stacking ensembles (LR, RF, HGB, XGB as base learners)
# Generates TWO reports: one for Voting, one for Stacking
# Outputs: ensemble_voting.pkl, ensemble_stacking.pkl, label_encoder.pkl
python "Classical Machine Learning/Field Level/Ensemble - Voting and Stacking.py"

# SMOTE oversampling + meta-learner
# Run after Ensemble script (uses shared artifacts)
python "Classical Machine Learning/Field Level/SMOTE_meta.py"

# Pixel-level models (LR, RF, LightGBM, XGBoost) with field aggregation
# Trains all 4 models in a loop, generates one report per model
# Report includes: feature_importance.png for tree-based models
python "Classical Machine Learning/pixel_level/base_ml_models.py"

# Inference using trained ensemble models (run after Ensemble script)
python "Classical Machine Learning/Field Level/inference_classical_ensemble.py"
```

### Stage 2: Deep Learning (Pixel/Field-Level)

**Data required:** `merged_dl_258_259.parquet`

```bash 
cd deep_learn/src
# CNN+BiLSTM ensemble — BEST MODEL (Kappa ~0.77, F1 ~0.84)
# Trains 5 models with different seeds, 25 epochs each
# Uses Focal Loss + WeightedRandomSampler for class imbalance
# Saves model weights to models/new_model_seed_{0-4}_25epochs.pt
python "Deep Learning/Pixel_Field_Level/cnn_bilstm.py"

# Field-level evaluation of trained CNN+BiLSTM
# Run AFTER cnn_bilstm.py — loads saved weights and aggregates by FID
python "Deep Learning/Pixel_Field_Level/field_acc_cnnlstm.py"

# TabNet ensemble (5 models, field-level evaluation)
# Saves/loads models to saved_models_tabnet/tabnet_seed_{seed}.zip
# Skips training if saved model exists (delete .zip to retrain)
python "Deep Learning/Pixel_Field_Level/TabTransformer_Final_Field.py"

# 1D CNN hyperparameter search with Optuna (25 trials)
python "Deep Learning/Pixel_Field_Level/cnn_dl_hyper.py"

# Train 1D CNN with best Optuna params (run after cnn_dl_hyper.py)
python "Deep Learning/Pixel_Field_Level/best_ccn_params.py"
```

### Stage 3: Patch-Level Deep Learning

**Data required:** Generated by the first two scripts below.

```bash
# Step 1: Create patches from field boundaries (requires geopandas, rasterio)
MPLBACKEND=Agg python "Deep Learning/Patch Level/Create_Patches.py"

# Step 2: Extract pixel values per patch from GeoTIFFs
python "Deep Learning/Patch Level/Create Master Data.py"

# Step 3: Train 3D CNN (spectral-temporal convolutions on 128x128x10 patches)
# Input shape: (batch, T=10 months, 128, 128, 6 bands)
# Report includes: training_curves.png (loss/accuracy over 20 epochs)
python "Deep Learning/Patch Level/3D_CNN.py"

# Multi-Channel 2D CNN for spatial texture learning
python "Deep Learning/Patch Level/Multi_Channel_CNN.py"

# Ensemble combining patch-level models
python "Deep Learning/Patch Level/Ensemble - 3D CNN.py"

# Inference with trained ensemble (run after Ensemble - 3D CNN.py)
python "Deep Learning/Patch Level/Inference_Ensemble.py"
```

### Run All DL Models

A convenience script runs all deep learning models sequentially:

```bash
cd src

# Run all stages
python run_all_dl_models.py

# Run only pixel/field-level models (Stage 2)
python run_all_dl_models.py --stage 2

# Run only patch-level models (Stage 3)
python run_all_dl_models.py --stage 3

# Skip patch models if data doesn't exist
python run_all_dl_models.py --skip-patch

# Dry run (show what would execute)
python run_all_dl_models.py --dry-run
```

## Out-of-Sample Inference (Holdout Test Region)

The holdout test region (34S_20E_259N) contains 2,417 fields that were not used during training. Scripts in `out_of_sample/` generate predictions for competition submission.

### Data Pipeline

| Data | Source | Script |
| ---- | ------ | ------ |
| Pixel-level (DL) | `merged_dl_test_259N.parquet` | Pre-generated via `create_merged_dl_data.py` |
| Field-level (ML) | `combined_test_features.parquet` | Generated by `combine_test_parquets.py` |
| Patch-level | `test_patch_data.parquet` | Generated by `create_test_patches.py` |

### Running Inference

**Option 1: Run all steps automatically**

```bash
cd out_of_sample

# Run all available models (skips if outputs exist or models missing)
python run_all_inference.py

# Force re-run even if outputs exist
python run_all_inference.py --force

# Dry run - show what would execute
python run_all_inference.py --dry-run

# Run only specific models
python run_all_inference.py --models cnn_bilstm classical_ml
```

**Option 2: Run individual steps**

```bash
cd out_of_sample

# Step 1: Prepare data
python combine_test_parquets.py      # Classical ML (field-level features)
python create_test_patches.py        # 3D CNN (requires test GeoTIFFs)

# Step 2: Run inference
python inference_cnn_bilstm.py       # CNN-BiLSTM ensemble (ready now)
python inference_classical_ml.py     # Voting + Stacking (needs trained models)
python inference_tabnet.py           # TabNet ensemble (needs trained models)
python inference_3d_cnn.py           # 3D CNN (needs patch data + model)

# Step 3: Compare and create submission
python compare_predictions.py
```

### Output Files

| File | Description |
| ---- | ----------- |
| `predictions_voting.csv` | Voting ensemble predictions |
| `predictions_stacking.csv` | Stacking ensemble predictions |
| `predictions_cnn_bilstm.csv` | CNN-BiLSTM ensemble predictions |
| `predictions_tabnet.csv` | TabNet ensemble predictions |
| `predictions_3d_cnn.csv` | 3D CNN predictions |
| `../submissions/prediction.csv` | Final submission (selected model) |

### Model Dependencies

| Model | Required Files |
| ----- | -------------- |
| CNN-BiLSTM | `models/new_model_seed_{0-4}_25epochs.pt` |
| Classical ML | `models/ensemble_voting.pkl`, `ensemble_stacking.pkl`, `label_encoder.pkl` |
| TabNet | `saved_models_tabnet/tabnet_seed_{seed}.zip` |
| 3D CNN | `models/conv3d_time_patch_level2.h5` + test patch data |

## Model Outputs

All model artifacts are saved under `src/`:

| Directory              | Contents                                              |
| ---------------------- | ----------------------------------------------------- |
| `models/`              | PyTorch `.pt` weights, Keras `.h5`, `.pkl` artifacts  |
| `xgb_tuner/`           | XGBoost Optuna tuning results and saved models        |
| `saved_models_tabnet/` | TabNet model checkpoints (`.zip`)                     |
| `reports/`             | Auto-generated reports (see below)                    |

## Data Split Rules

- **Regional split:**
  - Training: regions `34S_19E_258N` and `34S_19E_259N` (2,436 fields)
  - Holdout test: region `34S_20E_259N` (2,417 fields) — used only for final inference
- **Field-level split:** Train/val/test splits are done on unique field IDs (`fid`) with `random_state=42`. All pixels from a field stay in the same partition to prevent data leakage.
- **Validation during training:** The train/val/test split within training regions is used for hyperparameter tuning and model selection. Final predictions are made on the holdout test region.

## License

This project is licensed under the MIT License.

### Contributors

- Devarsh Apurva Sheth, Disha Kacha, Sairam Venkatachalam (George Washington University)
